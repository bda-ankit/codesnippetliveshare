{"content": "from fake_useragent import UserAgent
from dateutil.parser import parse
from bs4 import BeautifulSoup
from datetime import datetime
import requests
import logging
import sqlite3
import pyodbc
import atexit
import httpx
import time
import os

Server_var = '192.168.50.96'                                              

ua = UserAgent()
userAgent = ua.random
print(userAgent)

dataBaseName = "ceta_org_za"
tableName = "ceta_org_za"

filePath = os.path.abspath(__file__)
logFilePath = fr"{os.path.expanduser('~')}\Documents\PythonLogs\{dataBaseName}.log"
logging.basicConfig(
    filename=logFilePath,
    level=logging.INFO,
    filemode='a',
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def exit_handler():
    try:
        logging.info("Program Completed!")
    except:
        logging.info('Program interrupted')
    finally:
        logging.info('Program Terminated!')
atexit.register(exit_handler)

from deep_translator import GoogleTranslator
def translate_text(text): 
        url = 'https://translation.googleapis.com/language/translate/v2'

        params = {
            'key': 'AIzaSyBDKD_SRcApQ2AUfbDPGQbz6O9mZN7W8FM',
            'q': text,
            'target': 'en'
        }

        response = requests.get(url, params=params)

        if response.status_code == 200:
            translation = response.json()['data']['translations'][0]['translatedText']
            return translation.replace('&quot;', '"')
        elif response.status_code == 403:
            return GoogleTranslator(source='auto', target='english').translate(text).strip()
        elif response.status_code == 400:
            return GoogleTranslator(source='auto', target='english').translate(text).strip()
        else:
            print('Error:', response.status_code, response.text)
            return text

conn = sqlite3.connect(f'{dataBaseName}.db')
logging.info(f'sqlite3 database {dataBaseName} connected')
cursor = conn.cursor()

cursor.execute(f'''CREATE TABLE IF NOT EXISTS {tableName}
                (Posting_id integer PRIMARY KEY autoincrement, tender_id text, date_c text,
                Email text, add1 text, add2 text, city text,
                state text, pincode text, country text,
                url text, tel text, fax text,
                contact_person text, maj_org text,
                tender_notice_no text, notice_type text, ind_classification text, global text, mfa text,
                tender_details text, short_desc text, est_cost text, currency text, doc_cost text, doc_start text,
                doc_last text, open_date text, earnest_money text, Financier text, tender_doc_file text, sector text, corregendum text, corregendum_details text,
                project_name text, source text, t_side text, entry_date text,
                TPosting_Id text, Download_File text, Modification_File text, cpv text, AddConsultant text, AddContractor text, PrjPhase text, PrjSubPhase text,
                PrjCapacity text, FacilityType text, PeriodContract text, Events text, bidderSection text, language text, Proctype text, PrjStatus text, doc_purchase_start_date text,
                doc_purchase_end_date text, prebid_meeting_date text, emd_value text,
                emd_currency text, doc_cost_currency text, bid_bond_amt text, bid_bond_currency text,
                eligibility_criteria text, procurement_type text, tender_issue_date text, private_tender text,
                free_tender text, parent_id text, latitude text, longitude text, designation text, doc_open_date text, Content text, otherDocs text, DupFlag integer)''')

logging.info(f'Table {tableName} Created and Connected!')

url = 'https://www.ceta.org.za/'

sourceNewName = f"{url.split('://')[1].split('/')[0]}py"
def download_file_dir(url):
    files_dir = os.path.expanduser('~') + "\\Documents\\" + "PythonDocuments\\" + url.split('://')[1].split("/")[0] + "py\\" + "Files\\"
    if os.path.exists(files_dir):
        pass
    else:
        os.makedirs(files_dir)
    return files_dir
files_dir = download_file_dir(url)
logging.info(f"Document Download Directory created: {files_dir}")

from zipfile import ZipFile
def download_files(links):
    try:
        all_links = links.split(", ")
        fullname = os.path.join(files_dir, datetime.now().strftime(f"%d%m%Y_%H%M%S%f"))

        if len(all_links) > 1:
            fullname += ".zip"
            zip_obj = ZipFile(fullname, "w")
            for file_link in all_links:
                file_extension = get_file_extension(file_link)
                filename = os.path.join(files_dir, f"{datetime.now().strftime('%d%m%Y_%H%M%S%f')}{file_extension}")
                headers = {"User-Agent": userAgent}
                
                response = requests.get(file_link, headers=headers, timeout=20)

                with open(filename, "wb") as file:
                    file.write(response.content)

                zip_obj.write(filename, os.path.basename(filename))
                os.remove(filename)
            logging.info("Files Downloaded and Zipped Successfully")

        else:
            file_link = all_links[0]
            file_extension = get_file_extension(file_link)
            fullname += file_extension
            headers = {"User-Agent": f"{userAgent}"}
            response = requests.get(file_link, headers=headers, timeout=20)
            file = open(fullname, "wb")
            time.sleep(2)
            file.write(response.content)
            file.close()
            logging.info("File Downloaded")

        return fullname

    except Exception as e:
        print(e)
        logging.error("Error occurred: " + str(e))

def get_file_extension(file_link):
    try:
        file_extension = file_link.rsplit('.', 1)[-1]
        return f".{file_extension}"
    except:
        print('No Extension Found')

# def serverData():
#     try:
#         msSQL = conn.execute(f"""SELECT * from {tableName} where DupFlag = 0""")
#         logging.info(f"Select Fresh Data from {tableName} Table")

#         if Server_var == '192.168.50.96':
#             sqltable = "Tenders_tendersinfo2"
#             sql_conn = pyodbc.connect('Driver={SQL Server};'
#                                         'Server=192.168.50.96;'
#                                         'Database=VisualWebRipperGlobalManual;'
#                                         'UID=tenders;' 'PWD=12train89;')
#         else:
#             sqltable = "tenders"
#             sql_conn = pyodbc.connect('DRIVER={SQL Server};SERVER=%s;DATABASE=tidemasters;UID=sp;PWD=b2bind;' % (Server_var))

#         logging.info(f"Selected table is {sqltable}")

#         for row in msSQL:
#             sql_cursor = sql_conn.cursor()
#             sql_cursor.execute("Exec [dbo].[USP_GetNewKey]")
#             global Posting_Id
#             Posting_Id = sql_cursor.fetchone()
            
#             TPosting_Id = "T" + str(Posting_Id[0])

#             livecolumnName = 'EMail, add1, Country, URL, Tel, Maj_Org, tender_notice_no, notice_type, ind_classification, global, mfa, Tenders_details, short_desc, doc_last, Download_File, otherDocs, source, Content, Posting_Id'
#             livequeCount = ', '.join(['?'] * len(livecolumnName.split(', ')))
#             query = f'''INSERT INTO {sqltable} ({livecolumnName}) VALUES ({livequeCount})'''
#             valueList = [EMail, add1, Country, URL, Tel, Maj_Org, tender_notice_no, notice_type, ind_classification, Global, mfa, tenderDetails, short_desc, doc_last, Download_File, otherDocs, source, Content, Posting_Id[0],]
#             sql_cursor.execute(query, valueList)
#             sql_conn.commit()
#             sql_cursor.close()
#             sql_conn.close()

#             print(f'MSSQL Server Data Inserted into {sqltable} where TPosting ID: {TPosting_Id}')
#             logging.info(f'MSSQL Server Data Inserted into {sqltable} where TPosting ID: {TPosting_Id}')

#             conn.execute(f"UPDATE {tableName} SET DupFlag = 1 WHERE DupFlag = 0")
#             conn.commit()
#             conn.close()

#     except Exception as e:
#         logging.error(f'An error occurred: {str(e)}', exc_info=True)

try:
    url = 'https://www.ceta.org.za/tenders/open-tenders-rfqs'
    res = httpx.get(url, verify=False, headers={"User-Agent": userAgent}, timeout=20)
    soup = BeautifulSoup(res.content, 'html.parser')
    logging.info(f'{url} visited')

    rows = soup.find_all('div', attrs={'class': 'panel panel-default tender-panel'})

    for row in rows[:]:
        tender_notice_no = row.find(string="BID NO: ").find_next('span').text.strip()
        details = row.find('h4', {'class': 'panel-title expand'}).text.strip().splitlines()
        short_desc = details[0].capitalize()
        DupFlag = 0
        conn = sqlite3.connect(f'{dataBaseName}.db')
        cursor = conn.cursor()
        query = f"SELECT * FROM {tableName} WHERE short_desc = '{short_desc}' and tender_notice_no = '{tender_notice_no}'"
        cursor.execute(query)
        data = cursor.fetchone()
        if data is None:

            EMail = 'scmtenders1@ceta.co.za'
            add1 = '52 14th Road, Noordwyk, Midrand, 1687'
            html1 = str(row)
            Country = 'South Africa'
            URL = 'www.ceta.org.za'
            Tel = '+27 11 265 5901'
            Maj_Org = 'CONSTRUCTION EDUCATION AND TRAINING AUTHORITY (CETA)'
            notice_type = 1
            ind_classification = '0100710, 0104710, 0106710, 0107710'
            Global, mfa = 2, 0

            # tenderDetails = f'Tenders are invited for: {short_desc}'
            
            # try:
            #     doc_last_temp_t =  [index for index in details if 'Closing' in index]
            #     doc_last_temp = doc_last_temp_t[0].split(': ')[1]
            #     doc_last_temp_obj = parse(doc_last_temp, dayfirst=False)
            #     doc_last = doc_last_temp_obj.strftime('%d/%m/%Y')
            # except:
            #     doc_last =''

            # curr_date = datetime.now().date()
            # if doc_last != '':
            #     date_obj_date = doc_last_temp_obj.date()
            #     if date_obj_date > curr_date:
            #         source = sourceNewName
            #     else:
            #         source = sourceNewName
            # else:
            #     source = sourceNewName

            # try:
            #     allhtml = html1
            #     allhtml = allhtml.replace('\n', '')
            #     allhtml = allhtml.replace('\t', '')
            # except:
            #     pass

            # Google_translate = """<div id="google_translate_element"></div>
            #                         <script type="text/javascript">
            #                             function googleTranslateElementInit() {
            #                                 new google.translate.TranslateElement(
            #                                 {pageLanguage: 'auto',
            #                                 autoDisplay: false,
            #                                 layout:     google.translate.TranslateElement.InlineLayout.SIMPLE},
            #                                 'google_translate_element'
            #                                 );
            #                             }
            #                         </script>
            #                         <script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit">
            #                         </script> """

            # final_html = Google_translate + "<br /><br />" + allhtml + "<br /><br />"

            # soup = BeautifulSoup(final_html, "lxml")

            # for m in soup.find_all('a'):
            #     m.replaceWithChildren()
            # try:
            #     for sp in soup.find_all('img'):
            #         sp.decompose()
            # except:
            #     pass

            # Content = str(soup)

            # # Download_File = None
            # # doc_download_list = []

            # try:
            #     links = row.find_all('a', {'class': 'btn btn-orange download-btn btn-document'})
            #     full_url = []
            #     selected_urls = []
            #     for link in links:
            #         pdfurl = link['href']
            #         full_url.append(f'https://www.ceta.org.za/{pdfurl}')
            #     doc_download_list = full_url
            #     for url in full_url:
            #         if len(url) <= 800:
            #             selected_urls.append(url)
            #     Download_File = ", ".join(selected_urls[:])

            # except Exception as e:
            #     Download_File = None
            #     doc_download_list = []
            #     logging.error(f'An error occurred: {str(e)}', exc_info=True)

            # if Download_File:
            #     try:
            #         otherDocs = download_files(Download_File)
            #         if otherDocs == None:
            #                 otherDocs = ''
            #         logging.info("Document :-  %s\n", otherDocs)
            #     except Exception as e:
            #         print(e)
            #         otherDocs = ''
            #         logging.info("Document is not Available :- %s\n", e)
            # else:
            #     otherDocs = ''

            # litecolumnName = 'Email, add1, country, url, tel, maj_org, tender_notice_no, notice_type, ind_classification, global, mfa, tender_details, short_desc, doc_last, Download_File, otherDocs, source, Content, DupFlag'
            # litequeCount = ', '.join(['?'] * len(litecolumnName.split(', ')))
            # cursor.execute(f"INSERT OR IGNORE INTO {tableName} ({litecolumnName}) VALUES ({litequeCount})",
            #     (EMail, add1, Country, URL, Tel, Maj_Org, tender_notice_no, notice_type, ind_classification, Global, mfa, tenderDetails, short_desc, doc_last, Download_File, otherDocs, source, Content, DupFlag))
            # conn.commit()
            # print(f"New Data: {short_desc} commited to sqlite3")
            # logging.info(f"New Data: {short_desc} commited to sqlite3")
            # serverData()

        else:
            print(f"Duplicate Data: {short_desc}...skipped")  
            logging.info(f"Duplicate Data: {short_desc}...skipped")  

    print("All Data Fetched Successfully, No More Page Available")
    logging.info("All Data Fetched Successfully, No More Page Available")

    conn.close()

except Exception as e:
    print(str(e))", "timestamp": 1724255182.4435947}
